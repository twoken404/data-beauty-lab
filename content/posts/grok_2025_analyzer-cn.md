---
title: "æˆ‘çš„Grokå¹´åº¦æŠ¥å‘Šï¼šä¸€ä½è§†è§‰åˆ›ä½œè€…çš„AIä½¿ç”¨åˆ†æ"
date: 2025-12-31
slug: "grok-annual-report-chinese-analysis"
categories: ["äººå·¥æ™ºèƒ½", "æ•°æ®åˆ†æ", "åˆ›æ„å·¥ä½œ"]
tags: ["Grok", "AIä½¿ç”¨åˆ†æ", "å¹´åº¦æŠ¥å‘Š", "å›¾åƒç”Ÿæˆ", "æ•°æ®å¯è§†åŒ–", "Python", "AIåˆ›ä½œ"]
draft: false
---

# Grok å¹´åº¦ä½¿ç”¨åˆ†ææŠ¥å‘Š
> æœ¬æ–‡ä½¿ç”¨çš„æ•°æ®åˆ†æä»£ç å·²å¼€æºï¼Œä»“åº“åœ°å€ï¼š  
> [https://github.com/twoken404/grok_and_deepseek_2025_analyze/](https://github.com/twoken404/grok_and_deepseek_2025_analyze/)
> 
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: 2025-12-31 21:07:50*

## ğŸ“Š æ€»ä½“æ¦‚å†µ

- **æ€»å¯¹è¯æ•°**: 65 æ¬¡
- **æ€»æ¶ˆæ¯æ•°**: 844 æ¡
- **ä½ çš„æ¶ˆæ¯**: 415 æ¡
- **AIå›å¤**: 429 æ¡
- **æ¶ˆæ¯æ¯”ä¾‹**: ä½ :415 : 429 AI

## ğŸ¨ å›¾åƒç”Ÿæˆç»Ÿè®¡

- **å›¾åƒç”Ÿæˆæ¬¡æ•°**: 366 æ¬¡
- **å›¾åƒç”Ÿæˆç‡**: 43.4% (å æ‰€æœ‰æ¶ˆæ¯æ¯”ä¾‹)
- **å«å›¾åƒçš„å¯¹è¯**: 58 ä¸ª
- **å¹³å‡æç¤ºè¯é•¿åº¦**: 495 å­—ç¬¦

## ğŸ¤– æ¨¡å‹ä½¿ç”¨æƒ…å†µ

- **grok-4**: 108 æ¬¡ (12.8%)
- **grok-4-1-non-thinking-w-tool**: 514 æ¬¡ (60.9%)
- **grok-4-auto**: 109 æ¬¡ (12.9%)
- **grok-3**: 111 æ¬¡ (13.2%)
- ****: 2 æ¬¡ (0.2%)

### æ¨¡å¼ä½¿ç”¨
- **expertæ¨¡å¼**: 97 æ¬¡
- **grok-4-1æ¨¡å¼**: 483 æ¬¡
- **autoæ¨¡å¼**: 198 æ¬¡

## â° ä½¿ç”¨æ—¶é—´æ¨¡å¼

### æ—¶æ®µåˆ†å¸ƒ
- æ·±å¤œ (0-6ç‚¹):  15 (1.8%)
- æ—©æ™¨ (6-9ç‚¹):  4 (0.5%)
- ä¸Šåˆ (9-12ç‚¹): â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 103 (12.2%)
- ä¸­åˆ (12-14ç‚¹): â–ˆâ–ˆâ–ˆâ–ˆ 68 (8.1%)
- ä¸‹åˆ (14-18ç‚¹): â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 165 (19.5%)
- æ™šä¸Š (18-22ç‚¹): â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 383 (45.4%)
- æ·±å¤œ (22-24ç‚¹): â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 106 (12.6%)

### æ˜ŸæœŸåˆ†å¸ƒ
- å‘¨ä¸€: 187 æ¡
- å‘¨äºŒ: 103 æ¡
- å‘¨ä¸‰: 78 æ¡
- å‘¨å››: 117 æ¡
- å‘¨äº”: 90 æ¡
- å‘¨å…­: 131 æ¡
- å‘¨æ—¥: 138 æ¡

## ğŸ’¬ å¯¹è¯ç±»å‹åˆ†å¸ƒ

- **å›¾åƒç”Ÿæˆ**: 58 æ¬¡ (89.2%)
- **æŠ€æœ¯è®¨è®º**: 3 æ¬¡ (4.6%)
- **æ™®é€šå¯¹è¯**: 1 æ¬¡ (1.5%)
- **çŸ¥è¯†é—®ç­”**: 2 æ¬¡ (3.1%)
- **åˆ›æ„å†™ä½œ**: 1 æ¬¡ (1.5%)

## ğŸ”‘ å›¾åƒæç¤ºçƒ­é—¨å…³é”®è¯

1. **warm**: 350 æ¬¡
2. **cinematic**: 241 æ¬¡
3. **chinese**: 191 æ¬¡
4. **light**: 161 æ¬¡
5. **her**: 161 æ¬¡
6. **ratio**: 157 æ¬¡
7. **aspect**: 155 æ¬¡
8. **lighting**: 146 æ¬¡
9. **style**: 143 æ¬¡
10. **film**: 138 æ¬¡
11. **from**: 136 æ¬¡
12. **color**: 134 æ¬¡
13. **shot**: 129 æ¬¡
14. **soft**: 125 æ¬¡
15. **woman**: 121 æ¬¡
16. **man**: 119 æ¬¡
17. **atmosphere**: 113 æ¬¡
18. **golden**: 111 æ¬¡
19. **his**: 104 æ¬¡
20. **field**: 100 æ¬¡

## ğŸ·ï¸ å¯¹è¯æ ‡é¢˜å…³é”®è¯

- cinematic: 25 æ¬¡
- scene: 12 æ¬¡
- image: 10 æ¬¡
- generation: 9 æ¬¡
- chinese: 9 æ¬¡
- hong: 7 æ¬¡
- kong: 7 æ¬¡
- romance: 3 æ¬¡
- tension: 3 æ¬¡
- tense: 3 æ¬¡

## ğŸ’¡ ä½¿ç”¨æ€»ç»“

### ğŸ¨ è§†è§‰åˆ›ä½œè€…å‹
- ä¸»è¦ä½¿ç”¨Grokè¿›è¡Œå›¾åƒç”Ÿæˆ
- æç¤ºè¯è¯¦ç»†ï¼Œè¿½æ±‚è‰ºæœ¯æ•ˆæœ
- è§†è§‰åˆ›æ„è¡¨è¾¾ä¸°å¯Œ

### â° æ—¶é—´åå¥½: æ™šä¸Š (18-22ç‚¹)

### ğŸ“ åŸºäºæ ·æœ¬æ•°æ®çš„è§‚å¯Ÿ
ä»æä¾›çš„æ ·æœ¬çœ‹ï¼Œä½ çš„ä½¿ç”¨ç‰¹ç‚¹åŒ…æ‹¬ï¼š
- è¯¦ç»†çš„åœºæ™¯æè¿°èƒ½åŠ›
- å¯¹è§†è§‰ç»†èŠ‚çš„é«˜åº¦å…³æ³¨
- å–œæ¬¢è®¾å®šç‰¹å®šæƒ…å¢ƒï¼ˆæˆ˜æ—¶ã€æµªæ¼«ç­‰ï¼‰
- ä½¿ç”¨ä¸“å®¶æ¨¡å¼è¿½æ±‚é«˜è´¨é‡è¾“å‡º

## å¹´åº¦æŠ¥å‘Šç”Ÿæˆä»£ç 

```python
import json
from datetime import datetime
from collections import Counter, defaultdict
import re

class GrokDataAnalyzer:
    def __init__(self, json_file_path):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        with open(json_file_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        # æå–æ‰€æœ‰å¯¹è¯
        self.conversations = self.data.get('conversations', [])
        
    def parse_timestamp(self, timestamp_data):
        """è§£ææ—¶é—´æˆ³"""
        if isinstance(timestamp_data, dict) and '$date' in timestamp_data:
            # å¤„ç†MongoDBæ—¥æœŸæ ¼å¼
            date_data = timestamp_data['$date']
            if isinstance(date_data, dict) and '$numberLong' in date_data:
                timestamp_ms = int(date_data['$numberLong'])
                return datetime.fromtimestamp(timestamp_ms / 1000)
            elif isinstance(date_data, str):
                return datetime.fromisoformat(date_data.replace('Z', '+00:00'))
        elif isinstance(timestamp_data, str):
            return datetime.fromisoformat(timestamp_data.replace('Z', '+00:00'))
        return None
    
    def analyze_basic_stats(self):
        """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
        total_conversations = len(self.conversations)
        total_messages = 0
        human_messages = 0
        assistant_messages = 0
        image_generations = 0
        
        # ç»Ÿè®¡æ¨¡å‹ä½¿ç”¨æƒ…å†µ
        model_usage = Counter()
        mode_usage = Counter()
        
        # æ—¶é—´ç›¸å…³ç»Ÿè®¡
        date_stats = defaultdict(lambda: {'messages': 0, 'images': 0})
        hour_stats = defaultdict(int)
        
        for conv_data in self.conversations:
            conversation = conv_data.get('conversation', {})
            responses = conv_data.get('responses', [])
            
            # å¯¹è¯åˆ›å»ºæ—¶é—´
            create_time_str = conversation.get('create_time')
            if create_time_str:
                conv_date = self.parse_timestamp(create_time_str)
                if conv_date:
                    date_key = conv_date.strftime('%Y-%m-%d')
            
            for resp in responses:
                response_data = resp.get('response', {})
                total_messages += 1
                
                # å‘é€è€…ç»Ÿè®¡
                sender = response_data.get('sender', '').lower()
                if 'human' in sender:
                    human_messages += 1
                elif 'assistant' in sender:
                    assistant_messages += 1
                
                # æ¨¡å‹ç»Ÿè®¡
                model = response_data.get('model', 'unknown')
                model_usage[model] += 1
                
                # æ¨¡å¼ç»Ÿè®¡
                metadata = response_data.get('metadata', {})
                request_meta = metadata.get('request_metadata', {})
                mode = request_meta.get('mode', 'default')
                mode_usage[mode] += 1
                
                # å›¾åƒç”Ÿæˆç»Ÿè®¡
                if response_data.get('query_type') == 'imagine':
                    image_generations += 1
                
                # æ—¶é—´åˆ†æ
                create_time = self.parse_timestamp(response_data.get('create_time'))
                if create_time:
                    # æŒ‰æ—¥æœŸç»Ÿè®¡
                    date_key = create_time.strftime('%Y-%m-%d')
                    date_stats[date_key]['messages'] += 1
                    if response_data.get('query_type') == 'imagine':
                        date_stats[date_key]['images'] += 1
                    
                    # æŒ‰å°æ—¶ç»Ÿè®¡
                    hour_key = create_time.hour
                    hour_stats[hour_key] += 1
        
        return {
            'total_conversations': total_conversations,
            'total_messages': total_messages,
            'human_messages': human_messages,
            'assistant_messages': assistant_messages,
            'image_generations': image_generations,
            'image_generation_rate': (image_generations / total_messages * 100) if total_messages > 0 else 0,
            'model_usage': dict(model_usage),
            'mode_usage': dict(mode_usage),
            'date_stats': dict(date_stats),
            'hour_stats': dict(hour_stats)
        }
    
    def analyze_image_generations(self):
        """åˆ†æå›¾åƒç”Ÿæˆæƒ…å†µ"""
        image_prompts = []
        image_count_by_conv = defaultdict(int)
        
        for conv_data in self.conversations:
            conv_id = conv_data.get('conversation', {}).get('id', 'unknown')
            responses = conv_data.get('responses', [])
            
            for resp in responses:
                response_data = resp.get('response', {})
                if response_data.get('query_type') == 'imagine':
                    query = response_data.get('query', '')
                    image_prompts.append(query)
                    image_count_by_conv[conv_id] += 1
        
        # åˆ†ææç¤ºè¯ç‰¹å¾
        prompt_features = {
            'total_prompts': len(image_prompts),
            'avg_prompt_length': sum(len(p) for p in image_prompts) / len(image_prompts) if image_prompts else 0,
            'common_keywords': self.extract_keywords(image_prompts),
            'conversations_with_images': len(image_count_by_conv),
            'images_per_conversation': dict(Counter(image_count_by_conv.values()))
        }
        
        return prompt_features
    
    def extract_keywords(self, prompts, top_n=20):
        """ä»æç¤ºè¯ä¸­æå–å…³é”®è¯"""
        all_text = ' '.join(prompts).lower()
        
        # æå–è‹±æ–‡å•è¯
        words = re.findall(r'\b[a-z]{3,}\b', all_text)
        
        # è¿‡æ»¤å¸¸è§åœç”¨è¯
        stop_words = {'the', 'and', 'with', 'for', 'this', 'that', 'are', 'was', 'were', 'has', 'have', 'had'}
        filtered_words = [w for w in words if w not in stop_words]
        
        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter(filtered_words)
        
        return dict(word_counts.most_common(top_n))
    
    def analyze_time_patterns(self):
        """åˆ†ææ—¶é—´æ¨¡å¼"""
        weekday_names = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        weekday_stats = {i: 0 for i in range(7)}
        month_stats = {i: 0 for i in range(1, 13)}
        time_slot_stats = {
            'æ·±å¤œ (0-6ç‚¹)': 0,
            'æ—©æ™¨ (6-9ç‚¹)': 0,
            'ä¸Šåˆ (9-12ç‚¹)': 0,
            'ä¸­åˆ (12-14ç‚¹)': 0,
            'ä¸‹åˆ (14-18ç‚¹)': 0,
            'æ™šä¸Š (18-22ç‚¹)': 0,
            'æ·±å¤œ (22-24ç‚¹)': 0
        }
        
        for conv_data in self.conversations:
            responses = conv_data.get('responses', [])
            for resp in responses:
                response_data = resp.get('response', {})
                create_time = self.parse_timestamp(response_data.get('create_time'))
                
                if create_time:
                    # æ˜ŸæœŸç»Ÿè®¡
                    weekday = create_time.weekday()  # 0=å‘¨ä¸€, 6=å‘¨æ—¥
                    weekday_stats[weekday] += 1
                    
                    # æœˆä»½ç»Ÿè®¡
                    month = create_time.month
                    month_stats[month] += 1
                    
                    # æ—¶æ®µç»Ÿè®¡
                    hour = create_time.hour
                    if 0 <= hour < 6:
                        time_slot_stats['æ·±å¤œ (0-6ç‚¹)'] += 1
                    elif 6 <= hour < 9:
                        time_slot_stats['æ—©æ™¨ (6-9ç‚¹)'] += 1
                    elif 9 <= hour < 12:
                        time_slot_stats['ä¸Šåˆ (9-12ç‚¹)'] += 1
                    elif 12 <= hour < 14:
                        time_slot_stats['ä¸­åˆ (12-14ç‚¹)'] += 1
                    elif 14 <= hour < 18:
                        time_slot_stats['ä¸‹åˆ (14-18ç‚¹)'] += 1
                    elif 18 <= hour < 22:
                        time_slot_stats['æ™šä¸Š (18-22ç‚¹)'] += 1
                    else:
                        time_slot_stats['æ·±å¤œ (22-24ç‚¹)'] += 1
        
        # è½¬æ¢æ˜ŸæœŸä¸ºä¸­æ–‡
        weekday_stats_named = {weekday_names[i]: weekday_stats[i] for i in range(7)}
        
        return {
            'weekday_stats': weekday_stats_named,
            'month_stats': month_stats,
            'time_slot_stats': time_slot_stats
        }
    
    def analyze_conversation_content(self):
        """åˆ†æå¯¹è¯å†…å®¹ç‰¹å¾"""
        conversation_types = Counter()
        title_keywords = []
        
        for conv_data in self.conversations:
            conversation = conv_data.get('conversation', {})
            responses = conv_data.get('responses', [])
            
            title = conversation.get('title', '').lower()
            title_keywords.append(title)
            
            # åˆ¤æ–­å¯¹è¯ç±»å‹
            has_images = any(
                resp.get('response', {}).get('query_type') == 'imagine'
                for resp in responses
            )
            
            if has_images:
                conversation_types['å›¾åƒç”Ÿæˆ'] += 1
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å®šå…³é”®è¯
                all_text = ' '.join([
                    resp.get('response', {}).get('message', '').lower() 
                    for resp in responses
                ])
                
                if any(keyword in all_text for keyword in ['ä»£ç ', 'ç¼–ç¨‹', 'python']):
                    conversation_types['æŠ€æœ¯è®¨è®º'] += 1
                elif any(keyword in all_text for keyword in ['è§£é‡Š', 'ä»€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•']):
                    conversation_types['çŸ¥è¯†é—®ç­”'] += 1
                elif any(keyword in all_text for keyword in ['åˆ›ä½œ', 'æ•…äº‹', 'è¯—æ­Œ']):
                    conversation_types['åˆ›æ„å†™ä½œ'] += 1
                else:
                    conversation_types['æ™®é€šå¯¹è¯'] += 1
        
        # æå–æ ‡é¢˜å…³é”®è¯
        all_titles = ' '.join(title_keywords)
        title_words = re.findall(r'\b[a-z]{3,}\b', all_titles.lower())
        title_word_counts = Counter(title_words)
        
        return {
            'conversation_types': dict(conversation_types),
            'common_title_words': dict(title_word_counts.most_common(10))
        }
    
    def generate_report(self, output_file='grok_å¹´åº¦åˆ†ææŠ¥å‘Š.md'):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        basic_stats = self.analyze_basic_stats()
        image_analysis = self.analyze_image_generations()
        time_patterns = self.analyze_time_patterns()
        content_analysis = self.analyze_conversation_content()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            # æ ‡é¢˜
            f.write("# Grok å¹´åº¦ä½¿ç”¨åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n\n")
            
            # 1. æ€»ä½“æ¦‚å†µ
            f.write("## ğŸ“Š æ€»ä½“æ¦‚å†µ\n\n")
            f.write(f"- **æ€»å¯¹è¯æ•°**: {basic_stats['total_conversations']} æ¬¡\n")
            f.write(f"- **æ€»æ¶ˆæ¯æ•°**: {basic_stats['total_messages']} æ¡\n")
            f.write(f"- **ä½ çš„æ¶ˆæ¯**: {basic_stats['human_messages']} æ¡\n")
            f.write(f"- **AIå›å¤**: {basic_stats['assistant_messages']} æ¡\n")
            f.write(f"- **æ¶ˆæ¯æ¯”ä¾‹**: ä½ :{basic_stats['human_messages']} : {basic_stats['assistant_messages']} AI\n\n")
            
            # 2. å›¾åƒç”Ÿæˆç»Ÿè®¡
            f.write("## ğŸ¨ å›¾åƒç”Ÿæˆç»Ÿè®¡\n\n")
            f.write(f"- **å›¾åƒç”Ÿæˆæ¬¡æ•°**: {basic_stats['image_generations']} æ¬¡\n")
            f.write(f"- **å›¾åƒç”Ÿæˆç‡**: {basic_stats['image_generation_rate']:.1f}% (å æ‰€æœ‰æ¶ˆæ¯æ¯”ä¾‹)\n")
            f.write(f"- **å«å›¾åƒçš„å¯¹è¯**: {image_analysis['conversations_with_images']} ä¸ª\n")
            f.write(f"- **å¹³å‡æç¤ºè¯é•¿åº¦**: {image_analysis['avg_prompt_length']:.0f} å­—ç¬¦\n\n")
            
            # 3. æ¨¡å‹ä½¿ç”¨æƒ…å†µ
            f.write("## ğŸ¤– æ¨¡å‹ä½¿ç”¨æƒ…å†µ\n\n")
            for model, count in basic_stats['model_usage'].items():
                f.write(f"- **{model}**: {count} æ¬¡ ({count/basic_stats['total_messages']*100:.1f}%)\n")
            
            if basic_stats['mode_usage']:
                f.write("\n### æ¨¡å¼ä½¿ç”¨\n")
                for mode, count in basic_stats['mode_usage'].items():
                    if mode != 'default':
                        f.write(f"- **{mode}æ¨¡å¼**: {count} æ¬¡\n")
            
            # 4. æ—¶é—´æ¨¡å¼åˆ†æ
            f.write("\n## â° ä½¿ç”¨æ—¶é—´æ¨¡å¼\n\n")
            
            # æ—¶æ®µåˆ†å¸ƒ
            f.write("### æ—¶æ®µåˆ†å¸ƒ\n")
            total_msgs = sum(time_patterns['time_slot_stats'].values())
            for slot, count in time_patterns['time_slot_stats'].items():
                if count > 0:
                    percentage = count / total_msgs * 100
                    bar = "â–ˆ" * int(percentage / 2)
                    f.write(f"- {slot}: {bar} {count} ({percentage:.1f}%)\n")
            
            # æ˜ŸæœŸåˆ†å¸ƒ
            f.write("\n### æ˜ŸæœŸåˆ†å¸ƒ\n")
            for day, count in time_patterns['weekday_stats'].items():
                f.write(f"- {day}: {count} æ¡\n")
            
            # 5. å¯¹è¯ç±»å‹åˆ†æ
            f.write("\n## ğŸ’¬ å¯¹è¯ç±»å‹åˆ†å¸ƒ\n\n")
            total_conv = sum(content_analysis['conversation_types'].values())
            for conv_type, count in content_analysis['conversation_types'].items():
                percentage = count / total_conv * 100 if total_conv > 0 else 0
                f.write(f"- **{conv_type}**: {count} æ¬¡ ({percentage:.1f}%)\n")
            
            # 6. çƒ­é—¨å…³é”®è¯
            if image_analysis['common_keywords']:
                f.write("\n## ğŸ”‘ å›¾åƒæç¤ºçƒ­é—¨å…³é”®è¯\n\n")
                for i, (keyword, count) in enumerate(image_analysis['common_keywords'].items(), 1):
                    f.write(f"{i}. **{keyword}**: {count} æ¬¡\n")
            
            # 7. æ ‡é¢˜å…³é”®è¯
            if content_analysis['common_title_words']:
                f.write("\n## ğŸ·ï¸ å¯¹è¯æ ‡é¢˜å…³é”®è¯\n\n")
                for word, count in content_analysis['common_title_words'].items():
                    f.write(f"- {word}: {count} æ¬¡\n")
            
            # 8. ä½¿ç”¨æ€»ç»“
            f.write("\n## ğŸ’¡ ä½¿ç”¨æ€»ç»“\n\n")
            
            # åˆ¤æ–­ä¸»è¦ç”¨é€”
            image_ratio = basic_stats['image_generation_rate']
            if image_ratio > 30:
                f.write("### ğŸ¨ è§†è§‰åˆ›ä½œè€…å‹\n")
                f.write("- ä¸»è¦ä½¿ç”¨Grokè¿›è¡Œå›¾åƒç”Ÿæˆ\n")
                f.write("- æç¤ºè¯è¯¦ç»†ï¼Œè¿½æ±‚è‰ºæœ¯æ•ˆæœ\n")
                f.write("- è§†è§‰åˆ›æ„è¡¨è¾¾ä¸°å¯Œ\n")
            elif 'å›¾åƒç”Ÿæˆ' in content_analysis['conversation_types']:
                f.write("### ğŸ­ æ··åˆä½¿ç”¨å‹\n")
                f.write("- å…¼é¡¾å›¾åƒç”Ÿæˆä¸æ–‡æœ¬å¯¹è¯\n")
                f.write("- æ—¢æœ‰åˆ›æ„è¡¨è¾¾ï¼Œä¹Ÿæœ‰å®ç”¨æŸ¥è¯¢\n")
                f.write("- ä½¿ç”¨åœºæ™¯å¤šæ ·åŒ–\n")
            else:
                f.write("### ğŸ’¬ æ–‡æœ¬å¯¹è¯å‹\n")
                f.write("- ä¸»è¦è¿›è¡Œæ–‡æœ¬äº¤æµ\n")
                f.write("- å…³æ³¨çŸ¥è¯†è·å–ä¸é—®é¢˜è§£å†³\n")
                f.write("- å¯¹è¯å†…å®¹ä¸°å¯Œ\n")
            
            # æ—¶é—´æ¨¡å¼åˆ¤æ–­
            top_time_slot = max(time_patterns['time_slot_stats'].items(), key=lambda x: x[1])[0]
            f.write(f"\n### â° æ—¶é—´åå¥½: {top_time_slot}\n")
            
            # æ ¹æ®ä½ çš„æ ·æœ¬æ•°æ®æ¨æµ‹
            f.write("\n### ğŸ“ åŸºäºæ ·æœ¬æ•°æ®çš„è§‚å¯Ÿ\n")
            f.write("ä»æä¾›çš„æ ·æœ¬çœ‹ï¼Œä½ çš„ä½¿ç”¨ç‰¹ç‚¹åŒ…æ‹¬ï¼š\n")
            f.write("- è¯¦ç»†çš„åœºæ™¯æè¿°èƒ½åŠ›\n")
            f.write("- å¯¹è§†è§‰ç»†èŠ‚çš„é«˜åº¦å…³æ³¨\n")
            f.write("- å–œæ¬¢è®¾å®šç‰¹å®šæƒ…å¢ƒï¼ˆæˆ˜æ—¶ã€æµªæ¼«ç­‰ï¼‰\n")
            f.write("- ä½¿ç”¨ä¸“å®¶æ¨¡å¼è¿½æ±‚é«˜è´¨é‡è¾“å‡º\n")
        
        print(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
        return output_file

# ä¸»ç¨‹åº
if __name__ == "__main__":
    try:
        print("æ­£åœ¨åˆ†æGrokä½¿ç”¨æ•°æ®...")
        analyzer = GrokDataAnalyzer('prod-grok-backend.json')  # è¯·å°†æ–‡ä»¶åæ›¿æ¢ä¸ºå®é™…æ–‡ä»¶å
        
        # ç”ŸæˆæŠ¥å‘Š
        report_file = analyzer.generate_report()
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_file}")
        
        # æ˜¾ç¤ºç®€è¦ç»Ÿè®¡
        stats = analyzer.analyze_basic_stats()
        print(f"\nğŸ“Š ç®€è¦ç»Ÿè®¡:")
        print(f"   å¯¹è¯æ€»æ•°: {stats['total_conversations']}")
        print(f"   æ¶ˆæ¯æ€»æ•°: {stats['total_messages']}")
        print(f"   å›¾åƒç”Ÿæˆ: {stats['image_generations']} æ¬¡")
        print(f"   æœ€å¸¸ç”¨æ¨¡å‹: {max(stats['model_usage'].items(), key=lambda x: x[1])[0]}")
        
    except FileNotFoundError:
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶")
        print("è¯·ç¡®ä¿ 'grok_data.json' æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹")
    except json.JSONDecodeError:
        print("âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
```